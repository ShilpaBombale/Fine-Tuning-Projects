# Fine-Tuning Projects Repository

This repository contains two fine-tuning projects that explore the application of transformer models in sentiment analysis and IC automation. Each project leverages state-of-the-art models and techniques for efficient fine-tuning.

## 1. **FT-Language-Models-for-Sentiment-Analysis**

This project focuses on fine-tuning language models for sentiment analysis on the IMDb review dataset using Hugging Face and parameter-efficient fine-tuning (PEFT) techniques such as LoRA (Low-Rank Adaptation).

### Models Used:
- **DistilBERT_base_uncased**: ~70M parameters
- **Albert_base_v2**: 11.8M parameters

### Tools & Libraries:
- **Hugging Face Transformers**: For model training and inference.
- **PEFT & LoRA**: To enable efficient fine-tuning with reduced computational resources.

### Task:
Fine-tuning the models for sentiment classification on IMDb reviews.

---

## 2. **FT-Llama-2-7b-on-Azure-AI-Studio**

This project involves fine-tuning the **LLaMA 2 7B** model on Azure AI Studio for integrated circuit (IC) automation tasks. The focus is on leveraging the capabilities of large language models for specialized automation tasks.

### Model Used:
- **LLaMA 2 7B**

### Platform:
- **Azure AI Studio**: Used for model training and deployment in a cloud environment.

### Task:
Fine-tuning the model to automate tasks related to IC design.


   
